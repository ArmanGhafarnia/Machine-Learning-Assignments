# -*- coding: utf-8 -*-
"""ML - 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15ORG3SuJlf_XfvSaVclZvccMKMH_RG2y
"""

import numpy as np
import pandas as pd
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import accuracy_score, f1_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from collections import Counter
import matplotlib.pyplot as plt
from sklearn import svm
from imblearn.over_sampling import RandomOverSampler
from sklearn.metrics import r2_score

data_set = pd.read_csv('/content/drive/MyDrive/ML - 2/insurance.csv')    #read table
data_set.head(10)

label_encoder = LabelEncoder()                                               # changing names to values  0,1,2,...
data_set['sex'] = label_encoder.fit_transform(data_set['sex'])

label_encoder = LabelEncoder()
data_set['smoker'] = label_encoder.fit_transform(data_set['smoker'])

label_encoder = LabelEncoder()
data_set['region'] = label_encoder.fit_transform(data_set['region'])




data_set.head(10)

onehot = OneHotEncoder(sparse=False)     # creat onehotencoder we want array not sparse matrix
sex = onehot.fit_transform(data_set[['sex']])  #convert smoke column to onehot
sex_column = pd.DataFrame(sex, columns=onehot.get_feature_names_out(['sex'])) #create new columns from type of smoking
data_set = data_set.drop('sex', axis=1)   #remove the related column     #axis -> column  labels - > row
data_set = pd.concat([sex_column, data_set], axis=1)  #add new columns to the table



onehot = OneHotEncoder(sparse=False)
smoker = onehot.fit_transform(data_set[['smoker']])
smoker_column = pd.DataFrame(smoker, columns=onehot.get_feature_names_out(['smoker']))
data_set = data_set.drop('smoker', axis=1)
data_set = pd.concat([smoker_column, data_set], axis=1)



onehot = OneHotEncoder(sparse=False)
region = onehot.fit_transform(data_set[['region']])
region_column = pd.DataFrame(region, columns=onehot.get_feature_names_out(['region']))
data_set = data_set.drop('region', axis=1)
data_set = pd.concat([region_column, data_set], axis=1)


data_set.head(10)

data_set['bmi'] = (data_set['bmi'] - data_set['bmi'].min())/(data_set['bmi'].max() - data_set['bmi'].min())
data_set.head(10)
# normalize values of bmi

borders = [0, 20, 40, 60, 80 , 100]
values = [0, 1, 2, 3, 4]
data_set['age'] = pd.cut(data_set['age'], bins=borders, labels=values)       #rand ages
data_set.head(10)

x = data_set.iloc[:,:-1]   #all inputs except last column(predict value)
y = data_set.iloc[:,-1]    #all rows in last column(predict value)

W = np.random.rand(x.shape[1])                        # first values of equation parameters
B = 0

"""x  --->  m * n   | y ---> m * 1    |  w ---> n * 1"""

def cost_function(x, y, W, B):
    m = x.shape[1]  # Number of training examples
    predictions = np.dot(x, W) + B
    squared_errors = (predictions - y) ** 2                          # cost function
    mean_squared_error = np.sum(squared_errors) / (2 * m)
    return mean_squared_error

x1_train, x1_test, y1_train, y1_test = train_test_split(x, y, test_size=0.1)  #split data_set for learning
x3_train, x3_test, y3_train, y3_test = train_test_split(x, y, test_size=0.3)  #split data_set for learning
x6_train, x6_test, y6_train, y6_test = train_test_split(x, y, test_size=0.5)  #split data_set for learning



loss1 = []
for i in range(1000):
  m = x1_train.shape[0]  # Number of training examples
  predictions = np.dot(x1_train, W) + B   # Calculate predictions
  W = W - 0.001 * np.dot(x1_train.T, predictions - y1_train) / m  # Update the value of W
  B = B - 0.001 *  np.mean(predictions - y1_train)  # Update the value of B
  loss1.append(cost_function(x1_train, y1_train, W, B))



loss3 = []
for i in range(1000):
  m = x3_train.shape[0]  # Number of training examples
  predictions = np.dot(x3_train, W) + B   # Calculate predictions
  W = W - 0.001 * np.dot(x3_train.T, predictions - y3_train) / m  # Update the value of W
  B = B - 0.001 *  np.mean(predictions - y3_train)  # Update the value of B
  loss3.append(cost_function(x3_train, y3_train, W, B))



loss6 = []
for i in range(1000):
  m = x6_train.shape[0]  # Number of training examples
  predictions = np.dot(x6_train, W) + B   # Calculate predictions
  W = W - 0.001 * np.dot(x6_train.T, predictions - y6_train) / m  # Update the value of W
  B = B - 0.001 *  np.mean(predictions - y6_train)  # Update the value of B
  loss6.append(cost_function(x6_train, y6_train, W, B))

plt.plot(range(1, 1001), loss1 , color = 'deeppink')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.title('Loss over Iterations')
plt.show()


plt.plot(range(1, 1001), loss3 , color = 'deeppink')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.title('Loss over Iterations')
plt.show()


plt.plot(range(1, 1001), loss6 , color = 'deeppink')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.title('Loss over Iterations')
plt.show()

predict = np.dot(x1_test, W) + B
r_square = r2_score(y1_test, predict)    # find r-squre error
print(r_square)

x = data_set.iloc[:,:-1]   #all inputs except last column(predict value)
y = data_set.iloc[:,-1]    #all rows in last column(predict value)

W = np.random.rand(x.shape[1])       # first values of equation parameters
B = 0

data_set['region_0_2'] = data_set["region_0"]**2
data_set['region_1_2'] = data_set["region_1"]**2
data_set['region_2_2'] = data_set["region_2"]**2
data_set['region_3_2'] = data_set["region_3"]**2

data_set['smoker_0_2'] = data_set["smoker_0"]**2
data_set['smoker_1_2'] = data_set["smoker_1"]**2            # add polonomials

data_set['sex_0_2'] = data_set["sex_0"]**2
data_set['sex_1_2'] = data_set["sex_1"]**2

data_set['bmi2'] = data_set["bmi"]**2

data_set['children2'] = data_set["children"]**2

x1_train, x1_test, y1_train, y1_test = train_test_split(x, y, test_size=0.1)  #split data_set for learning
x3_train, x3_test, y3_train, y3_test = train_test_split(x, y, test_size=0.3)  #split data_set for learning
x6_train, x6_test, y6_train, y6_test = train_test_split(x, y, test_size=0.5)  #split data_set for learning



loss1 = []
for i in range(1000):
  m = x1_train.shape[0]  # Number of training examples
  predictions = np.dot(x1_train, W) + B   # Calculate predictions
  W = W - 0.001 * np.dot(x1_train.T, predictions - y1_train) / m  # Update the value of W
  B = B - 0.001 *  np.mean(predictions - y1_train)  # Update the value of B
  loss1.append(cost_function(x1_train, y1_train, W, B))



loss3 = []
for i in range(1000):
  m = x3_train.shape[0]  # Number of training examples
  predictions = np.dot(x3_train, W) + B   # Calculate predictions
  W = W - 0.001 * np.dot(x3_train.T, predictions - y3_train) / m  # Update the value of W
  B = B - 0.001 *  np.mean(predictions - y3_train)  # Update the value of B
  loss3.append(cost_function(x3_train, y3_train, W, B))



loss6 = []
for i in range(1000):
  m = x6_train.shape[0]  # Number of training examples
  predictions = np.dot(x6_train, W) + B   # Calculate predictions
  W = W - 0.001 * np.dot(x6_train.T, predictions - y6_train) / m  # Update the value of W
  B = B - 0.001 *  np.mean(predictions - y6_train)  # Update the value of B
  loss6.append(cost_function(x6_train, y6_train, W, B))

plt.plot(range(1, 1001), loss1 , color = 'deeppink')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.title('Loss over Iterations')
plt.show()


plt.plot(range(1, 1001), loss3 , color = 'deeppink')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.title('Loss over Iterations')
plt.show()


plt.plot(range(1, 1001), loss6 , color = 'deeppink')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.title('Loss over Iterations')
plt.show()

predict = np.dot(x1_test, W) + B
r_square = r2_score(y1_test, predict)    # find r-squre error
print(r_square)